# NTHU_Natural_Language_Processing-IKMLab

This repository provides course materials of the Natural Language Processing course at NTHU (instructor: Prof. Hung-Yu Kao).

## Couse Materials

| Week | Topics | Slide | Video | Slido |
|:-:|---|:-:|:-:|:-:|
|W1 | 課程簡介 Syllabus / Introduction to NLP | [`Slide`](./Slides/W0_Syllabus.pdf) | [`Video1`](https://www.youtube.com/live/oZHHnuFVHtk) [`Video2`](https://www.youtube.com/live/PW-6nELZNhg)| [`Slido`](https://app.sli.do/event/wvKBbisHUsMsKXHH2wJfW3)
|W2 | 自然語言處理簡介 (1/2) Introduction to NLP (vector space, indexing, parts of speech, phrase structure) | [`Slide1`](./Slides/W1_NLP_brief.pdf) [`Slide2`](./Slides/W2_Word%20embeddings%20and%20Language%20Modeling%20(RNN).pdf)|  [`Video1`](https://www.youtube.com/live/jBKUDUbeOkE) [`Video2`](https://www.youtube.com/live/wmUOivdJb6M) | [`Slido`](https://app.sli.do/event/dKD1f7KsoB3JGYgeQ6SBkF)
|W3 | 自然語言處理簡介 (2/2) Introduction to NLP (Language model) | | [`Video`](https://youtube.com/live/4pvbArJ5JtM) | [`Slido`](https://app.sli.do/event/dKD1f7KsoB3JGYgeQ6SBkF)
|W4 | 基礎文字資料機器學習 (1/2) Basic machine learning for text (Text Classification, NB, NN) | [`Slide3-1`](./Slides/W3_Sequence-to-sequence%20Models%20and%20Attention%20Mechanisms.pdf) [`Slide3-2`](./Slides/W3_subword.pdf) [`Slide3-3`](./Slides/W3_Transformers.pdf) | [`Video1`](https://youtube.com/live/MNYs4Skugsk) [`Video2`](https://www.youtube.com/live/TdrAnUMqmA4) | [`Slido`](https://app.sli.do/event/dKD1f7KsoB3JGYgeQ6SBkF)
|W5 | 基礎文字資料機器學習 (2/2) Basic machine learning for text (word embedding, text representation) | [`Slide4`](./Slides/W4_bert_and_its_family.pdf) | [`Video`](https://youtube.com/live/stD4aeNi9o4) | [`Slido`](https://app.sli.do/event/dKD1f7KsoB3JGYgeQ6SBkF)
|W6 | 文字生成式AI簡介(1/3) Introduction to GAI (text): Word Embeddings, Language Modeling (RNN), Sequence-to-sequence Models, and Attention Mechanisms, Sub-word Tokenization; Transformers | | [`Video`](https://youtube.com/live/LxUTwOTnQFQ) | [`Slido`](https://app.sli.do/event/dKD1f7KsoB3JGYgeQ6SBkF) 
|W7 | *Python for text tutorial (1/2)* | [`SlideTA`](./Slides/pytorch_tutorial_NTHU_NLP.pdf) [`Slide5`](./Slides/W5_decoding.pdf) | [`VideoTA`](https://www.youtube.com/watch?v=sg22677pUEs) [`Video`](https://youtube.com/live/efrdB5Sqd7g) | [`Slido`](https://app.sli.do/event/dKD1f7KsoB3JGYgeQ6SBkF) 
|W8 | *Python for text tutorial (2/2)* | | [`VideoTA`](https://www.youtube.com/watch?v=VErSpYgZGiw) | [`Slido`](https://app.sli.do/event/dKD1f7KsoB3JGYgeQ6SBkF)
|W9 | 文字生成式AI簡介(2/3) Introduction to GAI (text): ELMo, BERT, GPT, and T5 (BERT and its Family) |
|W10| 文字生成式AI簡介(3/3) Introduction to GAI (text): Decoding Strategies and Evaluations for Natural Language Generation |
|W11| 大語言模型簡介與訓練 (1/3): Large language model concept and training (GPT-3, InstructGPT, RLHF) |
|W12| 大語言模型簡介與訓練 (2/3): Parameter Efficient Fine-Tuning (PEFT) |
|W13| 大語言模型簡介與訓練 (2/3): Introduction and Review technique of Retrieval Augmented Generation (RAG) |
|W14| **Term project presentation (1/3)** |
|W15| **Term project presentation (1/3)** |
|W16| **Term project presentation (1/3)** |
|W17| **Term Project (demo) (optional)** |
|W18| **Term Project (demo) (optional)** |

- Italics: *Tutorial*; Bold: **Reporting**
